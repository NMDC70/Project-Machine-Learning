---
title: "Machine Learning Project - Coursera"
author: "Nagesh Madhwal"
date: "November 18, 2015"
output: html_document
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE)
```

## Project Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. While people regularly quantify how much of a particular activity they do, they rarely quantify how well they do it. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset)

## Goal 

Use the Training data provided to train a Machine Learning Model to correctly predict the manner in which the test subjects did the activity. Twenty cases are provided in the Test data for which prediction output has to be submitted. A report has to be created on the approach & methodology including estimation of the out of sample error.

## Key Activities

Following are the activities that will be carried out in this project:

1) Feature Selection
2) Initial Evaluation of multiple models & shortlist to one with optimal number of features
3) Tune the chosen Model on the full Data Set
4) Submit Predictions


## Feature Selection

```{r eval= FALSE, echo = FALSE}
Fileurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
```

Dimensions of the Training data
```{r echo = FALSE}
Traindata <- read.csv("C:/FOR R Projects/pml-training.csv")
cat("Dimensions of the Training data", dim(Traindata))
```

Initial Exploratory analysis tells us that there are multiple columns with NA values, empty columns & columns with undefined values (DIV/0!). So we will remove these columns which have more than 80% NA, blank or undefined values from the Training Data. 

```{r echo = FALSE}
projtrain2 <- Traindata
projtrain2[projtrain2 == "#DIV/0!" | projtrain2 == ""] <- NA
inttrainset <- projtrain2[, !(colMeans(is.na(projtrain2)) > 0.5)]
inttrainset <- inttrainset[complete.cases(inttrainset), ]
cat("Dimensions after initial Data Cleansing", dim(inttrainset))
```

In step two we will remove redundancy by eliminating very highly correlated variables, we will remove non zero value predictors & the basic time stamp information. 

_"num_window" is a variable which has a very high correlation to "classe" because there are clear ranges of value in the data that correspond to specific values in "classe". If we have to tune our model to apply to data outside the given set then our ability to predict has to be based on a different sequencing of window numbers hence this variable is also dropped. Keeping "num_window"_ in the data set it can pretty much be the single predictor for activity as known value range of _num_window will give us the exact acitivty being performed. (For meeting the project requirements though we could have kept num_window_ as a predictor & created very accurate predictions with a very limited number of variables, or even just one variable!. With num_window and three other variables a Random Forest model gave 100% correct predictions.)

```{r echo= FALSE}
library(mlbench)
library(caret)
correlationMatrix <- cor(inttrainset[,8:59])
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
filteredDescr <- inttrainset[,-(highlyCorrelated + 7)]
filteredDescr1 <- filteredDescr[, -c(1,3,4,5)]
filteredDescr2 <- filteredDescr1[, - c(1,2,3)]
cat("final Dimensions of the Training Dataset", dim(filteredDescr2))
```

From this data set we will take a 20% subset for intial analysis. Using Recursive Feature Elimination we can see the relative importance of the different variables. These can also be an input for further optimizing number of features. We will also train Models for Random Forest, Learning Vector Quantization & GBM and evaluate the relative importance of the variables. 

The best approach would have been to compare the models with a 60/40 split of the training data but given the limitations of time & the computing capability that I have i am doing model comparison with 20 percent of data for training & then 20% of the data for testing.

```{r mychunk1, cache=TRUE, echo= FALSE }
library(foreach)
library(doParallel)
registerDoParallel(4)
intrain1 <- createDataPartition(y = filteredDescr2$classe, p = 0.2, list = FALSE)
myexpldata <- filteredDescr2[intrain1, ]
set.seed(1001)
modelrf <- train(classe ~ ., data = myexpldata, trControl = trainControl(method = "cv", number = 10))
importancerf <- varImp(modelrf, scale=FALSE)
set.seed(1001)
control <- rfeControl(functions=rfFuncs, method="cv", number=6)
results <- rfe(myexpldata[,1:31], myexpldata[,32], sizes=c(1:31), rfeControl=control)

par(mfrow=c(1,2))
plot(importancerf, main = "Importance of Variables - Random Forest")
plot(results, type=c("g", "o"), main = "Recursive Feature Elimination")
print(results)
```

```{r mychunk2, cache=TRUE, echo= FALSE}
set.seed(1001)
control <- trainControl(method="repeatedcv", number=10, repeats=3)
modellvq1 <- train(classe ~ ., data = myexpldata, method="lvq", trControl=control, tuneLength= 5)
set.seed(1001)
control <- trainControl(method="repeatedcv", number=10, repeats=3)
modellvq2 <- train(classe ~ ., data = myexpldata, method="lvq", preProcess="scale", trControl=control, tuneLength = 5)
importancelvq2 <- varImp(modellvq2, scale = FALSE)
set.seed(1001)
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
modelgbm <- train(classe ~., data = myexpldata, method = "gbm", trControl = fitControl, verbose = FALSE)
importancegbm <- varImp(modelgbm, scale = FALSE)
par(mfrow = c(1, 2))

plot(importancelvq2, main = "Importance of variables - Learning Vector Quantization")
plot(importancegbm, main = "Importance of variables - GBM")

```

Recursive feature elimination presents a 25 variable list for doing correct predictions. With num_window as a predictor RFE would return only one variable or a shorter list of 4 or 5 variables. 

The relative importance from other Models is also presented & as would be expected the ranking of importance is varying based on the models. Before taking any decision on pruning features further i want to have a look at out of sample errors with a test set created from our training data. 

```{r echo= FALSE}

BalanceData <- filteredDescr2[-intrain1, ]
inTest <- createDataPartition(y = BalanceData$classe, p = 0.2, list = FALSE)
myexpltestdata <- BalanceData[inTest, ]
predrf <- predict(modelrf, newdata = myexpltestdata)
compmytestRF <- myexpltestdata
compmytestRF$predright <- predrf == compmytestRF$classe
ErrorRF = 1- sum(compmytestRF$predright)/ nrow(compmytestRF)
cat("Number of correct predictions with Random Forest =", sum(compmytestRF$predright), "out of total cases =", nrow(compmytestRF))

predrfe <- predict(results, newdata = myexpltestdata)
compmytestRFE <- myexpltestdata
compmytestRFE$predright <- predrfe$pred == compmytestRFE$classe
ErrorRFE = 1- sum(compmytestRFE$predright)/ nrow(compmytestRFE)
cat("Number of correct predictions with RFE =", sum(compmytestRFE$predright), "out of total cases =", nrow(compmytestRFE))

```

```{r echo= FALSE}
predlvq1 <- predict(modellvq1, newdata = myexpltestdata)
compmytestLVQ1 <- myexpltestdata
compmytestLVQ1$predright <- predlvq1 == compmytestLVQ1$classe
cat("Number of correct predictions with LVQ w/o preprossing =", sum(compmytestLVQ1$predright), "out of total cases =", nrow(compmytestLVQ1))
ErrorLVQ1 = 1- sum(compmytestLVQ1$predright)/ nrow(compmytestLVQ1)
predlvq2 <- predict(modellvq2, newdata = myexpltestdata)
compmytestLVQ2 <- myexpltestdata
compmytestLVQ2$predright <- predlvq2 == compmytestLVQ2$classe
ErrorLVQ2 = 1- sum(compmytestLVQ2$predright)/ nrow(compmytestLVQ2)
cat("Number of correct predictions with LVQ with preprossing =", sum(compmytestLVQ2$predright), "out of total cases =", nrow(compmytestLVQ2))
predgbm <- predict(modelgbm, newdata = myexpltestdata)
compmytestgbm <- myexpltestdata
compmytestgbm$predright <- predgbm == compmytestgbm$classe
Errorgbm = 1- sum(compmytestgbm$predright)/ nrow(compmytestgbm)
cat("Number of correct predictions with GBM =", sum(compmytestgbm$predright), "out of total cases =", nrow(compmytestgbm))
cat("Out of Sample")
cat("Prediction Error Random Forest =", ErrorRF)
cat("Prediction Error RFE =", ErrorRFE)
cat("Prediction Error LVQ without Preprossing =", ErrorLVQ1)
cat("Prediction Error LVQ with preprosseing =", ErrorLVQ2)
cat("Prediction Error GBM =", Errorgbm)
```

Random Forest has better results than GBM for this data set. LVQ improves after preprocessing but even that result is way off, so we will select random forest as our model. For random Forest there seems to be significant contribution even by the least important variable so I will not prune the features any further (it might be worth doing two models one with 25 & one with 31 predictors but submission deadline is approaching & running one more model just takes too much time for the first run, so will do that at a later date!)

Final validation by creating Train & Test sets with a 60 / 40 split & training a Random Forest model. Before submission will train the model on the entire training data set (not showing that step here, the prediction output is the same in both cases). 


```{r echo= FALSE}
library(foreach)
library(doParallel)
registerDoParallel(4)
intrain2 <- createDataPartition(y = filteredDescr2$classe, p = 0.6, list = FALSE)
traindata <- filteredDescr2[intrain2, ]
set.seed(1001)
modelrf <- train(classe ~ ., data = traindata, trControl = trainControl(method = "cv", number = 10))
modelrf$finalModel
testdata <- filteredDescr2[-intrain2, ]
predictrf <- predict(modelrf, newdata = testdata)
testdata$predright <- predictrf == testdata$classe
cat("Number of correct predictions =", sum(testdata$predright), "out of total cases =", nrow(testdata))

Error = 1- sum(testdata$predright)/ nrow(testdata)
cat("Out of Sample Prediction error =", Error )
ftestdata <- read.csv("C:/FOR R Projects/pml-testing.csv")
predictfinal <- predict(modelrf, newdata = ftestdata)
predictfinal

```
20/20


# Annexure

The code base of the project 
```{r eval=FALSE }
# caching chunks of code & parallel processing are possibly the most important tools for doing this project!!!
## Caching though also gives issues as here I am caching output of 6 machine learning models & that creates its own complication.

Traindata <- read.csv("C:/FOR R Projects/pml-training.csv")
cat("Dimensions of the Training data", dim(Traindata)
    
projtrain2 <- Traindata
projtrain2[projtrain2 == "#DIV/0!" | projtrain2 == ""] <- NA
inttrainset <- projtrain2[, !(colMeans(is.na(projtrain2)) > 0.5)]
inttrainset <- inttrainset[complete.cases(inttrainset), ]
cat("Dimensions after initial Data Cleansing", dim(inttrainset))

library(mlbench)
library(caret)
correlationMatrix <- cor(inttrainset[,8:59])
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
filteredDescr <- inttrainset[,-(highlyCorrelated + 7)]
filteredDescr1 <- filteredDescr[, -c(1,3,4,5)]
filteredDescr2 <- filteredDescr1[, -2]
cat("final Dimensions of the Training Dataset", dim(filteredDescr2))


library(foreach)
library(doParallel)
registerDoParallel(4)
intrain1 <- createDataPartition(y = filteredDescr2$classe, p = 0.2, list = FALSE)
myexpldata <- filteredDescr2[intrain1, ]
set.seed(1001)
modelrf <- train(classe ~ ., data = myexpldata, trControl = trainControl(method = "cv", number = 10))
importancerf <- varImp(modelrf, scale=FALSE)
set.seed(1001)
control <- rfeControl(functions=rfFuncs, method="cv", number=3)
results <- rfe(myexpldata[,1:33], myexpldata[,34], sizes=c(1:33), rfeControl=control)
control <- trainControl(method="repeatedcv", number=10, repeats=3)
set.seed(1001)
control <- trainControl(method="repeatedcv", number=10, repeats=3)
modellvq1 <- train(classe ~ ., data = myexpldata, method="lvq", trControl=control, tuneLength= 5)
set.seed(1001)
control <- trainControl(method="repeatedcv", number=10, repeats=3)
modellvq2 <- train(classe ~ ., data = myexpldata, method="lvq", preProcess="scale", trControl=control, tuneLength = 5)
importancelvq2 <- varImp(modellvq2, scale = FALSE)
set.seed(1001)
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
modelgbm <- train(classe ~., data = myexpldata, method = "gbm", trControl = fitControl, verbose = FALSE)
importancegbm <- varImp(modelgbm, scale = FALSE)
par(mfrow = c(2, 2))
plot(importancerf, main = "Importance of Variables - Random Forest")
plot(results, type=c("g", "o"), main = "Recursive Feature Elimination")
plot(importancelvq2, main = "Importance of variables - Learning Vector Quantization")
plot(importancegbm, main = "Importance of variables - GBM")
cat("Top Four variables identified by Recursive Feature Elemination", "\n" , predictors(results))
print(results)

BalanceData <- filteredDescr2[-intrain, ]
inTest <- createDataPartition(y = BalanceData$classe, p = 0.2, list = FALSE)
myexpltestdata <- BalanceData[inTest, ]
predrf <- predict(modelrf, newdata = myexpltestdata)
compmytestRF <- myexpltestdata
compmytestRF$predright <- predrf == compmytestRF$classe
ErrorRF = 1- sum(compmytestRF$predright)/ nrow(compmytestRF)
cat("Number of correct predictions with Random Forest =", sum(compmytestRF$predright), "out of total cases =", nrow(compmytestRF))
predrfe <- predict(results, newdata = myexpltestdata)
compmytestRFE <- myexpltestdata
compmytestRFE$predright <- predrfe$pred == compmytestRFE$classe
ErrorRFE = 1- sum(compmytestRFE$predright)/ nrow(compmytestRFE)
cat("Number of correct predictions with RFE =", sum(compmytestRFE$predright), "out of total cases =", nrow(compmytestRFE))
predlvq1 <- predict(modellvq1, newdata = myexpltestdata)
compmytestLVQ1 <- myexpltestdata
compmytestLVQ1$predright <- predlvq1 == compmytestLVQ1$classe
cat("Number of correct predictions with LVQ w/o preprossing =", sum(compmytestLVQ1$predright), "out of total cases =", nrow(compmytestLVQ1))
ErrorLVQ1 = 1- sum(compmytestLVQ1$predright)/ nrow(compmytestLVQ1)
predlvq2 <- predict(modellvq2, newdata = myexpltestdata)
compmytestLVQ2 <- myexpltestdata
compmytestLVQ2$predright <- predlvq2 == compmytestLVQ2$classe
ErrorLVQ2 = 1- sum(compmytestLVQ2$predright)/ nrow(compmytestLVQ2)
cat("Number of correct predictions with LVQ with preprossing =", sum(compmytestLVQ2$predright), "out of total cases =", nrow(compmytestLVQ2))
predgbm <- predict(modelgbm, newdata = myexpltestdata)
compmytestgbm <- myexpltestdata
compmytestgbm$predright <- predgbm == compmytestgbm$classe
Errorgbm = 1- sum(compmytestgbm$predright)/ nrow(compmytestgbm)
cat("Number of correct predictions with GBM =", sum(compmytestgbm$predright), "out of total cases =", nrow(compmytestgbm))
cat("Prediction Error Random Forest =", ErrorRF)
cat("Prediction Error RFE =", ErrorRFE)
cat("Prediction Error LVQ without Preprossing =", ErrorLVQ1)
cat("Prediction Error LVQ with preprosseing =", ErrorLVQ2)
cat("Prediction Error GBM =", Errorgbm)

intrain2 <- createDataPartition(y = filteredDescr2$classe, p = 0.6, list = FALSE)
traindata <- filteredDescr2[intrain2, ]
set.seed(1001)
modelrf <- train(classe ~ ., data = traindata, trControl = trainControl(method = "cv", number = 10))
modelrf$finalModel
testdata <- filteredDescr2[-intrain2, ]
predictrf <- predict(modelrf, newdata = testdata)
testdata$predright <- predictrf == testdata$classe
cat("Number of correct predictions =", sum(testdata$predright), "out of total cases =", nrow(testdata))

Error = 1- sum(testdata$predright)/ nrow(testdata)
cat("Out of Sample Prediction error =", Error )
ftestdata <- read.csv("C:/FOR R Projects/pml-testing.csv")
predictfinal <- predict(modelrf, newdata = ftestdata)
predictfinal

==================

```





